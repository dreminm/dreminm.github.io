<p>You can read the Methodology below or download the presentation in pdf format 
    <a href="https://drive.google.com/file/d/15eQz5nBPGU91nwV7BMWEFnRbiweXqh91/view?usp=sharing">here</a>.<br> You also can see it in 
    Google Slides <a href="https://docs.google.com/presentation/d/1_Qgw9g_TyJrDWh6bWTYAh58TuygqWNqr0CpP_4HFBBw/edit?usp=sharing">here</a>.</p>

<h3 id="problem_definition">Problem definition</h3>
<p>Super Resolution is the process of calculating high-resolution samples from their low-resolution counterparts. 
    Working with images we can utilize natural preferences and make a high-resolution image, which is only in a way similar to the real one. With video, 
    we can explore additional information from neighboring frames and restore details from the original scene. Our benchmark is aimed to find the best algorithms 
    for the restoration of real details during Video Super Resolution processing.</p>

<h3 id="dataset">Dataset</h3>
<h4 id="content_types">Content types</h4>
<p>To analyze models’ ability to restore real details, we have built a test-stand (see Figure 1), which includes 
    different hard patterns for video restoration tasks:</p>
<div class="center">
    <div>
        <img width="50%" src="/assets/img/benchmarks/vsr/parts.jpg">
        <p><i>Figure 1. The test-stand of the benchmark.</i></p>
    </div>
</div>
<p>To calculate metrics on particular types of content and verify how models work with different input, we divide each output 
    frame into parts by detection of crosses:<br>
<b>Part 1. “Board”</b> includes a few small objects and photos of people's faces*. We want to see models’ results on textures with small details. The striped fabric and 
balls of yarn may produce a Moire pattern (see figure 2). Restoration of people's faces is important for video surveillance.<br>
<b>Part 2. “QR-codes”</b> consists of several QR-codes of different sizes to find the size of the smallest one which can be detected in the models' output frame. 
In a low-resolution frame, QR-code patterns may be blended. Thus models have difficulties restoring patterns.<br>
<b>Part 3. “Text”</b> includes two types of text: handwriting and a set of symbols. It’s difficult to put all these difficult parts in the training dataset. So, 
all of them are new for the model and it needs to restore them.<br>
<b>Part 4. “Metal paper”</b> contains foil that was intensively crumpled. It’s interesting because of light reflection, which strongly but periodically changes between frames.<br>
<b>Part 5. “Color lines”</b> is a printed image with a large number of thin color lines. This is difficult for models because thin 
    lines of similar colors are mixed in a low-resolution frame.<br>
<b>Part 6. ‘Car numbers”</b> consists of a set of car number plates of different countries and different sizes**. This content type is important for the video 
surveillance field and dashcams development.<br>
<b>Part 7. “Noise”</b> includes difficult noise patterns. Models cannot restore real ground-truth noise and each one produces a unique pattern.<br>
<b>Part 8. “Mira”</b> includes difficult patterns for video restoration: a set of straight and curved lines of different thicknesses and in different directions.<br>
*Photos were generated by <sup><a href="#references">[1]</a></sup>.<br>
**Car numbers are generated randomly and printed on paper.</p>
<div class="center">
    <div>
        <video autoplay loop muted playsinline width="75%">
            <source src="/assets/img/benchmarks/vsr/muar.av1.mp4" type='video/mp4'>
            <source src="/assets/img/benchmarks/vsr/muar.vp9.webm" type='video/webm'>
            <source src="/assets/img/benchmarks/vsr/muar.x264.mp4" type='video/mp4'>
        </video>
        <p><i>Figure 2. Example of a Moire pattern on the “Board”.</i></p>
    </div>
</div>
<h4 id="content_types">Motion types</h4>
<p>The dataset includes three videos with different types of motion:
    <ul>
        <li><b>Hand tremor</b> — video shooting from the fixed point without a tripod (the photographer holds the camera in his hands). Because of the natural tremor of hands, there is a random small motion in frames
        </li>
        <div class="center">
                <img src = "/assets/img/benchmarks/vsr/motion1.png" />
                <video autoplay loop muted playsinline>
                    <source src="/assets/img/benchmarks/vsr/test1.av1.mp4" type='video/mp4'>
                    <source src="/assets/img/benchmarks/vsr/test1.vp9.webm" type='video/webm'>
                    <source src="/assets/img/benchmarks/vsr/test1.x264.mp4" type='video/mp4'>
                </video>
        </div>
        <li><b>Parallel motion</b> — the camera is moving from side to side in parallel with test-stand</li>
        <div class="center">
                <img src = "/assets/img/benchmarks/vsr/motion2.png" />
                <video autoplay loop muted playsinline>
                    <source src="/assets/img/benchmarks/vsr/test2.av1.mp4" type='video/mp4'>
                    <source src="/assets/img/benchmarks/vsr/test2.vp9.webm" type='video/webm'>
                    <source src="/assets/img/benchmarks/vsr/test2.x264.mp4" type='video/mp4'>
                </video>
        </div>
        <li><b>Rotation</b> — the camera is moving from side to side in a half-circle</li>
        <div class="center">
                <img src = "/assets/img/benchmarks/vsr/motion3.png" />
                <video autoplay loop muted playsinline>
                    <source src="/assets/img/benchmarks/vsr/test3.av1.mp4" type='video/mp4'>
                    <source src="/assets/img/benchmarks/vsr/test3.vp9.webm" type='video/webm'>
                    <source src="/assets/img/benchmarks/vsr/test3.x264.mp4" type='video/mp4'>
                </video>
        </div>
    </ul>
</p>
<h4 id="technical">Technical characteristics of the camera</h4>
<p>Dataset was prepared with Canon EOS 7D. We take fast series of photos and consider it as a sequence of video frames. We store each video as a sequence of 
    frames in PNG format, which were converted from JPG by FFmpeg. The output of a model is also a sequence of frames, which we compare with GT sequence of 
    frames to verify the model's performance. Camera’s settings:<br>
    ISO – 4000<br>
    aperture – 400<br>
    resolution – 5184x3456
    </p>
<h4 id="dataset_preparation">Dataset preparation</h4>
<p>
    <ul>
        <li><b>Source video</b> has resolution 5184x3456 and was stored in sRGB color space. Each video’s length is 100 frames.</li>
        <li><b>Ground-truth</b>. Each video was degraded by bicubic interpolation to generate GT with resolution 1920x1280. It’s essential 
            because many open-source models don’t have available code to process a large frame. Processing large frame is also time-consuming.</li>
        <li>Then <b>input video</b> was degraded from GT in two ways: bicubic interpolation (BI) and Gaussian blurring and downsampling (BD).</li>
    </ul>
</p>
<h4 id="noise_input">Noise input</h4>
<p>To verify how a model works with noisy data, we prepared noise counterparts for each input video. To generate realistic noise, we use python implementation <sup><a href="#references">[2]</a></sup>
    of the noise model proposed in CBDNet by Liu et al <sup><a href="#references">[3]</a></sup>. We need to set two parameters: one for the Poisson part of the noise and another for the Gauss part of 
    the noise.<br>
To estimate the level of real noise in our camera, we set a camera on a tripod and capture a sequence of 100 frames from a fixed point. Then we average the sequence 
to estimate a clean image. Thus we gain hundred of real noise examples. Then we chose parameters for generated noise so that the distributions of generated and real 
noise are similar (see Figure 3). Our parameters choice: sigma_s = 0.001, sigma_c = 0.035.</p>
<div class="center">
    <div style="width:75%">
        <img src="/assets/img/benchmarks/vsr/noise.jpg">
        <p><i>Figure 3. The distribution of real and generated noise.</i></p>
    </div>
</div>
<p>Finally, we have 12 tests:</p>
<div class="center">
    <div style="width:90%">
        <img src="/assets/img/benchmarks/vsr/test_names.png">
    </div>
</div>
<h3 id="metrics">Metrics</h3>
<h4 id="psnr">PSNR</h4>
<p>PSNR – commonly used metric based on pixels’ similarity. We noticed that a model, trained on one degradation type and tested on another type, can generate 
    frames with a global shift relative to GT (see Figure 4). Thus we checked integer shifts from [-3,3] in both axes and choose the shift with maximal PSNR value. 
    This maximal value is considered as a metric result in our benchmark.</p>
<div class="center">
    <div style="width:75%">
        <video autoplay loop muted playsinline>
            <source src="/assets/img/benchmarks/vsr/shift.av1.mp4" type='video/mp4'>
            <source src="/assets/img/benchmarks/vsr/shift.vp9.webm" type='video/webm'>
            <source src="/assets/img/benchmarks/vsr/shift.x264.mp4" type='video/mp4'>
        </video>
        <p><i>Figure 4. On the left: The same crop from the model’s output and GT frame.<br>
            On the right: PSNR visualization for this crop.</i></p>
    </div>
</div>
<p>We chose PSNR-Y because it’s more efficient than PSNR-RGB. Meanwhile, a correlation between these metrics is high. For metric calculation, we use the 
    implementation from skimage.metrics<sup><a href="#references">[4]</a></sup>. A higher metric value indicates better quality. The metric value for GT is infinite.</p>
<h4 id="ssim">SSIM</h4>
<p>SSIM – another commonly used metric based on structure similarity. A shift of frames can influence this metric too. Thus we tried to find the optimal 
    shift similarly to PSNR calculation and noticed that optimal shifts for these metrics can differ, but not more than 1 pixel in any axis. Because SSIM 
    has large computational complexity, we decided to find optimal shift not among all shifts, but near with optimal shift for PSNR (in a distance of 1 pixel 
    in any axis). We calculate SSIM on the Y channel of the YUV color space. For metric calculation, we use the implementation from skimage.metrics<sup><a href="#references">[5]</a></sup>. A higher 
    metric value indicates better quality. The metric value for GT is 1.</p>
<h4 id="erqa">ERQAv1.0</h4>
<p>ERQAv1.0 (Edge Restoration Quality Assessment, version 1.0) estimates how well a model has restored edges of the high-resolution frame. Firstly, we find
    edges in both output and GT frames. To do it we use OpenCV implementation<sup><a href="#references">[6]</a></sup> of the Canny algorithm<sup><a href="#references">[7]</a></sup>. A threshold for the initial finding of strong edges 
    is set to 200. And a threshold for edge linking is set to 100. These coefficients allow to highlight edges of all subjects even of small sizes but skip lines, 
    which are not important (see Figure 5).</p>
<div class="center">
    <div style="width:90%">
        <img src="/assets/img/benchmarks/vsr/edge1.jpg">
        <p><i>Figure 5. An example of edges, highlighted by the chosen algorithm</i></p>
    </div>
</div>
<p>Then we compare these edges by using F1-score. To compensate a one-pixel shift of edge, which is not essential for human perception of objects, we consider 
    as true-positive pixels of output’s edges, which are not in edges of GT but are near (on the difference of one pixel) with the edge of GT(see Figure 6). 
    A higher metric value indicates better quality. The metric value for GT is 1.
</p>
<div class="center">
    <div style="width:90%">
        <img src="/assets/img/benchmarks/vsr/edge2.jpg">
        <p><i>Figure 6. Visualization of F1-score, used for edges comparison</i></p>
    </div>
</div>
<h4 id="qrcr">QRCRv1.0</h4>
<p>QRCRv1.0 (QR-Codes Restoration, version 1.0) finds the smallest size (in pixels) of QR-code, which can be detected in output frames of a model. 
    To project metric values on [0,1], we consider a relation of the smallest QRs’ sizes for GT and output frame (see Figure 7). If in the model’s result 
    we can’t detect any QR-code, the metric value is set to 0. A higher metric value indicates better quality. The metric value for GT is 1.
</p>
<div class="center">
    <div style="width:60%">
        <img src="/assets/img/benchmarks/vsr/qr.jpg">
        <p><i>Figure 7. Example of detected crosses in output and GT frame.<br>
            The metric value for the output frame is 0.65</i></p>
    </div>
</div>
<h4 id="crrm">CRRMv1.0</h4>
<p>CRRMv1.0 (Colorfullness Reduced-Reference Metric, version 1.0) – calculate colorfullness* in both frames and compare them. To calculate colorfullness we 
    use metric, proposed by Hasler et al.<sup><a href="#references">[8]</a></sup>. Comparison of colorfullness levels is performed as a relation between colorfullness in GT frame and output frame. 
    Then to project metric on [0,1] and penalize both increasing and decreasing of colorfullness, we take the absolute difference between 1 and the relation and 
    then subtract it from 1. A higher metric value indicates better quality. The metric value for GT is 1.<br>
    *Colorfulness measures how colorful an image is: if it’s bright and has a lot of different colors.
    </p>

<h3 id="metrics_acc">Metrics accumulation</h3>
<p>Because each model can work differently on different content types, we consider metric values not only on full-frame but also on parts with different content.
    To do this we detect crosses in frames and calculate coordinates of all parts from them.<br>
    Crosses in some frames are distorted and cannot be detected. Thus we choose keyframes, where we can detect all crosses and calculate metrics only on these 
    keyframes. We noticed that metrics values on these keyframes are highly correlated and choose the mean of values through keyframes as a final metric value 
    for each test case.</p>

<h3 id="subjective">Subjective comparison</h3>
<p>We cut the sequence of 30 frames and convert them to video with fps 8 by FFmpeg. Then we crop 10 small pieces from the video and conduct a side-by-side 
    subjective comparison for all these pieces by subjectify.us. Each participant sees twenty-five paired videos and has <b>to choose a video with better-restored 
    details</b> in each pair (the option “indistinguishable” is also available). Three of these videos are considered as verification questions*. The rest answers of
    successful participants are used to predict the ranking using the Bradley-Terry model.<br>
    *Answers to verification questions are not included in the final result.</p>

<h3 id="fps">The computational complexity of models</h3>
<p>We tested each model using NVIDIA Titan RTX and measured runtime on the same test sequence:
    <ul>
        <li>Test case — parallel motion + BD degradation + with noise</li>
        <li>100 frames</li>
        <li>Input resolution — 480×320</li>
    </ul>
    FPS is calculated as the execution time of a full model runtime divided by the number of sequence frames.
</p>

<h3 id="references">References</h3>
<ol>
    <li><a href="https://thispersondoesnotexist.com">https://thispersondoesnotexist.com</a></li>
    <li><a href="https://github.com/yzhouas/CBDNet_ISP">https://github.com/yzhouas/CBDNet_ISP</a></li>
    <li>Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo and Lei Zhang, "Toward Convolutional Blind Denoising of Real Photographs," 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 1712-1722, doi: 10.1109/CVPR.2019.00181.</li>
    <li><a href="https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.peak_signal_noise_ratio">https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.peak_signal_noise_ratio</a></li>
    <li><a href="https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.structural_similarity">https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.structural_similarity</a></li>
    <li><a href="https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga04723e007ed888ddf11d9ba04e2232de">https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga04723e007ed888ddf11d9ba04e2232de</a></li>
    <li><a href="https://en.wikipedia.org/wiki/Canny_edge_detector">https://en.wikipedia.org/wiki/Canny_edge_detector</a></li>
    <li>David Hasler and Sabine Suesstrunk, "Measuring Colourfulness in Natural Images," Proceedings of SPIE - The International Society for Optical Engineering, 2003, volume 5007, pp. 87-95, doi: 10.1117/12.477378.</li>
</ol>
