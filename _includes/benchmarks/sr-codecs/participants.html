<div id="comm_comp_div" class="datatable-container">
    <div class="datatable-center">
        <table id="char_table" class="display">
            <thead>
                <tr>
                    <th style="background-color: #3d6f96; color: #ffffff">Name</th>
                    <th style="background-color: #3d6f96; color: #ffffff">Number of models</th>
                    <th style="background-color: #3d6f96; color: #ffffff">Multi-frame</th>
                    <th style="background-color: #3d6f96; color: #ffffff">Upscale factor</th>
                    <th style="background-color: #3d6f96; color: #ffffff">Framework</th>
                    <th style="background-color: #3d6f96; color: #ffffff">Year</th>
                </tr>
            </thead>
            <tbody>
                <tr class="item">
                    <td><a href="#dbvsr">DBVSR</a></td>
                    <td>1</td>
                    <td style="color: #218838">Yes</td>
                    <td>4x</td>
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#dynavsr">DynaVSR-R</a></td>
                    <td>1</td>
                    <td style="color: #218838">Yes</td>
                    <td>4x</td>
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#egvsr">EGVSR</a></td>
                    <td>1</td>
                    <td style="color: #218838">Yes</td>
                    <td>4x</td>
                    <td>PyTorch</td>
                    <td>2021</td>
                </tr>
                <tr class="item">
                    <td><a href="#iSeeBetter">iSeeBetter</a></td>
                    <td>1</td>
                    <td style="color: #218838">Yes</td>
                    <td>2x, 4x</td>
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#lgfn">LGFN</a></td>
                    <td>1</td>
                    <td style="color: #218838">Yes</td>
                    <td>4x</td>
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#realsr">RealSR</a></td>
                    <td>1</td>
                    <td style="color: #d11d12">No</td>
                    <td>4x</td>
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#sof-vsr">SOF-VSR</a></td>
                    <td>2</td>
                    <td style="color: #218838">Yes</td>
                    <td>4x</td>
                    <td>PyTorch</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#topaz">Topaz Video Enhance AI</a></td>
                    <td>3</td>
                    <td style="color: #218838">Yes</td>
                    <td>2x, 4x</td>
                    <td>â€”</td>
                    <td>2020</td>
                </tr>
                <tr class="item">
                    <td><a href="#waifu2x">waifu2x-ncnn-vulkan</a></td>
                    <td>2</td>
                    <td style="color: #d11d12">No</td>
                    <td>2x, 4x</td>
                    <td>PyTorch</td>
                    <td>2018</td>
                </tr>
            </tbody>
        </table>

        <script>$(document).ready(function () {
            $('#char_table').DataTable({
                lengthChange: false,
                paging: false,
                searching: false
            });
        });</script>
    </div>
</div>

<h2 id="dbvsr">DBVSR</h2>
<ul>Estimate a motion blur for the particular input. Compensate the motion between frames explicitly.</ul>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/sr-codecs/dbvsr.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/cscss/DBVSR">GitHub</a>, <a href="https://arxiv.org/abs/2003.04716">paper</a></ul>

<h2 id="dynavsr">DynaVSR</h2>
<ul>Use meta-learning to estimate a degradation kernel for the particular input.</ul>
<ul>DynaVSR can be applied to any VSR deep-learning model. For our benchmark, we used pretrained weights for model EDVR, which use Deformable convolution to align neighboring frames.</ul>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/sr-codecs/dynavsr.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/esw0116/DynaVSR">GitHub</a>, <a href="http://arxiv.org/abs/2011.04482">paper</a></ul>

<h2 id="egvsr">EGVSR</h2>
<ul>The generator part 
    is  divided  into  FNet  module  and  SRNet  module  for  optical 
    flow estimation and video frame super-resolution, 
    respectively.</ul>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/sr-codecs/egvsr.png">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/Thmen/EGVSR">GitHub</a>, <a href="https://arxiv.org/ftp/arxiv/papers/2107/2107.05307.pdf">paper</a></ul>

<h2 id="iSeeBetter">iSeeBetter</h2>
<ul>A combination of an RNN-based optical flow method that preserves spatio-temporal information in the current and adjacent frames as the generator and a discriminator that is adept at ensuring the generated SR frame offers superior fidelity.</ul>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/sr-codecs/iseebetter.png">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/amanchadha/iSeeBetter">GitHub</a>, <a href="https://arxiv.org/pdf/2006.11161.pdf">paper</a></ul>

<h2 id="lgfn">LGFN</h2>
<ul>Use deformable convolutions with decreased multi-dilation convolution units (DMDCUs) to align frames explicitly. Fuse features from local and global fusion modules.</ul>
<div class="center">
	<div>
		<img src="/assets/img/benchmarks/sr-codecs/lgfn.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/BIOINSu/LGFN">GitHub</a>, <a href="https://doi.org/10.1109/ACCESS.2020.3025780">paper</a></ul>

<h2 id="realsr">RealSR</h2>
<ul>Try to estimate degradation kernel and noise distribution for better visual quality.</ul>
<div class="center">
	<div style="width:75%">
		<img src="/assets/img/benchmarks/sr-codecs/realsr.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/nihui/realsr-ncnn-vulkan">GitHub</a>, <a href="https://github.com/jixiaozhong/RealSR">GitHub</a>, <a href="https://doi.org/10.1109/CVPRW50498.2020.00241">paper</a></ul>

<h2 id="sof-vsr">SOF-VSR</h2>
<ul>Two models: SOF-VSR-BD (trained on gauss degradation type), SOF-VSR-BI (trained on bicubic degradation type)</ul>
<ul>Compensate motion by high-resolution optical flow, estimated from the low-resolution one in a coarse-to-fine manner.</ul>
<div class="center">
	<div style="width:75%">
		<img src="/assets/img/benchmarks/sr-codecs/sof-vsr.jpg">
	</div>
</div>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/LongguangWang/SOF-VSR">GitHub</a>, <a href="https://doi.org/10.1109/TIP.2020.2967596">paper</a></ul>

<h2 id="topaz">Topaz Video Enhance AI</h2>
<ul>Topaz Video Enhance AI is a commercial filter.</ul>
<ul>Three models: 
        <li>ahq-11 (Artemis High Quality v11) - upscale or sharpen high quality input video, reducing motion flicker, </li>
        <li>amq-12 (Artemis Medium Quality v12) - upscale or enhance medium quality video with moderate noise or compression artifacts, </li>
        <li>amqs-1 (Artemis Dehalo v1) - upscale or enhance medium quality progressive video that contains haloing, moderate noise or compression artifacts.</li></ul>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://www.topazlabs.com/video-enhance-ai">Website</a></ul>

<h2 id="waifu2x">waifu2x-ncnn-vulkan</h2>
<ul>Two models: waifu2x-anime and waifu2x-cunet</ul>
<ul>ncnn implementation of waifu2x converter. 
    waifu2x-ncnn-vulkan uses <a href="https://github.com/Tencent/ncnn">ncnn project</a> as the universal neural network inference framework.
</ul>
<ul>Added to the benchmark by MSU</ul>
<ul>Links: <a href="https://github.com/nihui/waifu2x-ncnn-vulkan">GitHub</a>, <a href="https://github.com/nagadomi/waifu2x">GitHub</a></ul>